{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topF\"></a>\n",
    "\n",
    "# Filtering out COS Data taken during the Day or Night\n",
    "\n",
    "# Learning Goals\n",
    "<font size=\"4 \"> This Notebook is designed to walk the user (<em>you</em>) through: <b>Filtering Cosmic Origins Spectrograph (<em>COS</em>) `TIME-TAG` data taken during the day from data taken during the night</b>:</font>\n",
    "    \n",
    "**1. [Processing a spectrum from a filtered dataset](#procF)**\n",
    "\n",
    "\\- 1.1. [Filtering the `TIME-TAG` data](#filtF)\n",
    "\n",
    "\\- 1.2. [Creating a new association file](#asnF)\n",
    "\n",
    "\\- 1.3. [Running the `CalCOS` pipeline on the filtered dataset](#CalCOSF)\n",
    "\n",
    "**2. [Comparing the filtered and unfiltered data](#compF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction and Setup\n",
    "**The Cosmic Origins Spectrograph ([*COS*](https://www.nasa.gov/content/hubble-space-telescope-cosmic-origins-spectrograph)) is an ultraviolet spectrograph on-board the Hubble Space Telescope([*HST*](https://www.stsci.edu/hst/about)) with capabilities in the near ultraviolet (*NUV*) and far ultraviolet (*FUV*).**\n",
    "\n",
    "**This tutorial aims to prepare you to work with the COS data of your choice by walking you through filtering `TIME-TAG` datapoints obtained by COS.**\n",
    "\n",
    "In particular, this tutorial will walk you through separating datapoints obtained during the Hubble Space Telescope's \"night\" - when the sun is below the geometric horizon from the observatory's point of view - from datapoints taken during the observatory's \"day\" - when the sun is above this horizon. \n",
    "You may wish to disaggregate photons from these two time periods, as data taken during the day can be subject to different and higher background noise conditions, as well as more intense geocoronal Lyman-alpha or Oxygen-I emission lines from the Earth’s atmosphere ([See Data Handbook 5.4.2](https://hst-docs.stsci.edu/cosdhb/chapter-5-cos-data-analysis/5-4-working-with-time-tag-data#id-5.4WorkingwithTIME-TAGData-5.4.2FilteringTime-TagData)).\n",
    "This type of data separation is possible with the [`TIME-TAG` data](https://hst-docs.stsci.edu/cosdhb/chapter-1-cos-overview/1-1-instrument-capabilities-and-design) obtained by the COS photon-counting detectors, because each individual encounter with a photon is recorded with its own metadata such as the time of the encounter, and can be linked to the physical position of Hubble at that time.\n",
    "\n",
    "- For an in-depth manual to working with COS data and a discussion of caveats and user tips, see the [COS Data Handbook](https://hst-docs.stsci.edu/display/COSDHB/).\n",
    "- For a detailed overview of the COS instrument, see the [COS Instrument Handbook](https://hst-docs.stsci.edu/display/COSIHB/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will import the following packages:\n",
    "\n",
    "- `costools timefilter` to select `TIME-TAG` datapoints by their metadata parameters\n",
    "- `calcos` to re-process the data\n",
    "- `numpy` to handle array functions\n",
    "- `astropy.io fits` and `astropy.table Table` for accessing FITS files\n",
    "- `glob` and `os` for working with system files\n",
    "- `matplotlib.pyplot` and `gridspec` for plotting data\n",
    "- `astroquery.mast Mast` and `Observations` for finding and downloading data from the [MAST](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html) archive\n",
    "- `pathlib Path` for managing system paths\n",
    "\n",
    "New versions of `CalCOS` are currently incompatible with astroconda. To create a Python environment capable of running all the data analyses in these COS Notebooks, please see Section 1 of our Notebook tutorial on [setting up an environment](https://github.com/spacetelescope/notebooks/blob/master/notebooks/COS/Setup/Setup.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for: filtering TIME TAG data i.e. by sun altitude\n",
    "from costools import timefilter\n",
    "\n",
    "# Import for: processing COS data\n",
    "import calcos \n",
    "\n",
    "# Import for: array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# Import for: reading fits files\n",
    "from astropy.io import fits                                            \n",
    "from astropy.table import Table\n",
    "\n",
    "# Import for: dealing with system files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Import for: plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# Import for: downloading the data\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "#Import for: working with system paths\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will also define a few directories we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will be important directories for the Notebook\n",
    "\n",
    "datadir = Path('./data')\n",
    "outputdir = Path('./output/')\n",
    "intermediate_dir = Path('./intermediate/')\n",
    "plotsdir = Path('./output/plots')\n",
    "\n",
    "# Make the directories if they don't exist\n",
    "datadir.mkdir(exist_ok=True), outputdir.mkdir(exist_ok=True), plotsdir.mkdir(exist_ok=True), intermediate_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we will need to download the data we wish to filter and analyze\n",
    "We choose the exposure with obs_id: `lbry01i6q`, because we happen to know it contains data taken both in the observatory's night and day and shows strong airglow lines. For more information on downloading COS data, see our [notebook tutorial on downloading COS data](https://github.com/spacetelescope/notebooks/blob/master/notebooks/COS/DataDl/DataDl.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_root = 'lbry01i6q'\n",
    "\n",
    "pl = Observations.get_product_list(Observations.query_criteria(proposal_id = '12604', obs_id = 'lbry01*')) # Create product list of all observations with that obs_id root\n",
    "pl_mask = ((pl['productSubGroupDescription'] == \"CORRTAG_A\") | (pl['productSubGroupDescription'] == \"CORRTAG_B\")) & (pl['obs_id'] == filename_root) # create mask with only the corrtag files\n",
    "Observations.download_products(pl[pl_mask], download_dir = str(datadir)) # Download those corrtag files\n",
    "\n",
    "file_locations_orig = glob.glob('./data/**/*corrtag*.fits', recursive=True)\n",
    "for file_orig in file_locations_orig:\n",
    "    os.rename(file_orig, './data/'+os.path.basename(file_orig)) # Aggregates the data from out of long convoluted filepaths\n",
    "    # NOTE - can cause problems if you have any non-unique filenames\n",
    "    #  for large datasets don't aggregate your data in this simple way that can't handle repeat names\n",
    "\n",
    "file_locations_a = glob.glob('./data/**/*corrtag_a.fits', recursive=True) # Finds all the FUVA files\n",
    "file_locations_b = glob.glob('./data/**/*corrtag_b.fits', recursive=True) # Finds all the FUVB files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = procF></a>\n",
    "# 1. Processing a spectrum from a filtered dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = filtF></a>\n",
    "## 1.1. Filtering the `TIME-TAG` data\n",
    "The `costools` package contains the `TimelineFilter` class, which - upon instantiation - filters by the parameters you give it. In other words, you don't have to run any functions or methods aside from instantiating the class. This is done by passing the following parameters to `timefilter.TimelineFilter`:\n",
    "- `input`: the path to the input `TIME-TAG` file. (string)\n",
    "- `output`: the path to the output `TIME-TAG` file. (string)\n",
    "- `filter`: a comparison filter (see examples below). Data points who meet the filter criteria will be *filtered out* by turning on their \"bad time interval\" data quality flag. (string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only `TIME-TAG` data has the timing data necessary to be processed using `timefilter`. We must first ensure that all the data we wish to process is `TIME-TAG` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_time_tag(filepath):\n",
    "    \"\"\"\n",
    "    Function which yields descriptive errors if the input file is not a COS TIME-TAG file.\n",
    "    Inputs: \n",
    "      filepath (str): path to your data\n",
    "    \"\"\"\n",
    "    instrume, imagetyp = fits.getval(filepath, keyword='INSTRUME'), fits.getval(filepath, keyword='IMAGETYP')\n",
    "    assert instrume == 'COS', \"DataTypeError: These files are not from COS\"\n",
    "    assert imagetyp == 'TIME-TAG', \"DataTypeError: These files are not TIME-TAG files. You cannot process them with timefilter.\"\n",
    "\n",
    "for i, file in enumerate(file_locations_a + file_locations_b):\n",
    "    check_time_tag(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Below, we will filter by the sun's altitude above the geometric horizon from the point-of-view of the Hubble Observatory. <b>Daytime</b> is whenever this altitude is greater than 0 degrees.</font>\n",
    "\n",
    "The filtering parameters are explained in the [Section 5.4.2 of the COS Data Handbook: Filtering Time-Tag Data](https://hst-docs.stsci.edu/cosdhb/chapter-5-cos-data-analysis/5-4-working-with-time-tag-data). \n",
    "\n",
    "*All of the available filters are*:\n",
    "\n",
    "|Property|Variable name|\n",
    "|-|-|\n",
    "|Time of photon encounter \\[32 ms bins\\]|`time`|\n",
    "|Observatory latitude/longitude|`longitude`/`latitude`|\n",
    "|Sun's altitude|`sun_alt`|\n",
    "|Sun-Earth-HST angle|`sun_zd`|\n",
    "|Observing target's current altitude|`target_alt`|\n",
    "|Radial velocity of HST towards target|`radial_vel`|\n",
    "|Shift along dispersion axis|`shift1`|\n",
    "|Total $\\frac{counts}{sec}$ in a box across the aperture about $Ly_\\alpha$|`ly_alpha`|\n",
    "|Total $\\frac{counts}{sec}$ in a box across the aperture about $O I 1304$|`oi_1304`|\n",
    "|Total $\\frac{counts}{sec}$ in a box across the aperture about $O I 1356$|`oi_1356`|\n",
    "|Detector dark current rate|`darkrate`|\n",
    "\n",
    "\n",
    "... or any combination of these filters. There is one more filter keyword which behaves slightly differently: `saa`. By specifying `saa <n>` where $0\\le n \\le 32$, you can exclude the $n^{th}$ model contour of the [South Atlantic Anomaly (SAA)](https://en.wikipedia.org/wiki/South_Atlantic_Anomaly) as modeled by `costools.saamodel`.\n",
    "\n",
    "The following comparison operators are allowed:\n",
    "\n",
    "|Symbol|Example|Meaning (\"Filter *out* points taken when...\")|\n",
    "|-|-|-|\n",
    "|`==`|`time == 0`|Time of photon interaction is equal to 0 seconds|\n",
    "|`!=`|`longitude != 3`|HST's longitude is not equal to 3˚|\n",
    "|`<=`|`sun_alt >= 0`|Sun's altitude is greater than or equal to 0˚|\n",
    "|`>=`|`target_alt <= 90`|Target's altitude is less than or equal to 90˚|\n",
    "|`<`|`radial_vel < 0.1`|Radial velocity of HST towards target is less than 10 $\\frac{km}{s}$|\n",
    "|`>`|`ly_a > 5`|Total $\\frac{counts}{sec}$ in a box across the aperture about $Ly_\\alpha$ is greater than 5 $\\frac{counts}{sec}$|\n",
    "||||\n",
    "|***Combination***|`oi_1304 > 100 AND sun_alt <= 10 OR target_alt < 30`|Total irradiance about 1304 Å is greater than 100 *AND* the Sun's altitude is less than or equal to 10˚ *OR* the target's altitude is below 30˚|\n",
    "\n",
    "*Note* that in the combination example above, the *\"OR\"* takes precedence over \"AND\". If a count does not satisfy both of the first 2 statements, but it does satisfy statement 3, it will be considered to meet the filter, its \"bad time interval\" data quality flag will be turned on, and it will not be allowed to contribute to the output spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* also that the `TimelineFilter` class will not overwrite existing files, so if you wish to filter the files using the following code more than once, you must delete the files you have already created in the `intermediate` directory. To ease this process, the first several lines in the following cell attempt to delete any previously created files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If the files already exist, delete them so you may run the filter again:\n",
    "if (intermediate_dir / \"filtered_corrtag_a.fits\").exists(): # Check if you have created a filtered FUVA file\n",
    "    print(f\"File exists; deleting {intermediate_dir / 'filtered_corrtag_a.fits'}\\n\")\n",
    "    (intermediate_dir / \"filtered_corrtag_a.fits\").unlink() # if so, delete the file so we can re-create it with TimelineFilter\n",
    "if (intermediate_dir / \"filtered_corrtag_b.fits\").exists(): # Repeat the check-delete process above with FUVB file\n",
    "    print(f\"File exists; deleting {intermediate_dir / 'filtered_corrtag_b.fits'}\\n\")\n",
    "    (intermediate_dir / \"filtered_corrtag_b.fits\").unlink()\n",
    "\n",
    "# Filter the files based on your criteria:\n",
    "for afile, bfile in zip(file_locations_a,file_locations_b): # This must be repeated for all exposure files, so loop through both FUVA and FUVB\n",
    "    timefilter.TimelineFilter(input=afile, output=str(intermediate_dir / \"filtered_corrtag_a.fits\"), # Run TimelineFilter on FUVA file\n",
    "                              filter = \"sun_alt > 0.\", verbose=True) # removes daytime data, i.e. where the Sun's altitude above the horizon is 0 degrees\n",
    "    timefilter.TimelineFilter(bfile, str(intermediate_dir / \"filtered_corrtag_b.fits\"), # Run TimelineFilter on FUVB file\n",
    "                                  \"sun_alt > 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = asnF></a>\n",
    "## 1.2. Creating a new association file\n",
    "In order to run the `CalCOS` pipeline on your newly filtered data, an association (`asn`) file must be made, instructing the pipeline where to look for the filtered `TIME-TAG` data files. Association files, (and how to create and edit them,) are discussed in detail in our [Notebook Tutorial on Association Files](https://github.com/spacetelescope/notebooks/blob/master/notebooks/COS/AsnFile/AsnFile.ipynb).\n",
    "\n",
    "In the next cells, we build **two** new association files from scratch:\n",
    "1. For these new **filtered** files\n",
    "2. For the original **unfiltered** files, so that we can compare and see the effect of filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create asn for the newly filtered-by-sun_alt files (this and next cell):\n",
    "\n",
    "# This dict properly assigns the type of exposure in a way that CalCOS will recognize\n",
    "type_dict = {'WAVECAL' : 'EXP-AWAVE',\n",
    "             'EXTERNAL/SCI' : 'EXP-FP'}\n",
    "\n",
    "files = glob.glob(str(intermediate_dir/\"*_corrtag_a.fits\")) # Finds all the corrtag_a (just FUVA) files downloaded above\n",
    "                                            # These file locations, names will differ depending on the output name above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note* that the above cell only found the segment A files** (i.e. from the COS FUVA detector.) You only need the A segment in the ASN, and CalCOS will find the associated B segment data for you, if the files are in the same directory.\n",
    "\n",
    "We can now build the `asn` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:    \n",
    "    # Adding the file details to the association table\n",
    "    rootnames = [f]  # MEMNAME\n",
    "    types = [type_dict[fits.getval(f, 'EXPTYPE')]] # MEMTYPE\n",
    "    included = [True] # MEMPRSNT\n",
    "    \n",
    "    # Adding the ASN details to the end of the association table\n",
    "    #  the rootname needs to be the full name, not just the rootname\n",
    "    asn_root = os.path.basename(f.split('corrtag')[0][:-1]) # remove the extra \"_\" at the end of string\n",
    "    rootnames.append(asn_root.upper()) # MEMNAME\n",
    "    types.append('PROD-FP') # MEMTYPE\n",
    "    included.append(True) # MEMPRSNT\n",
    "    \n",
    "    # Putting together the fits table\n",
    "    #   40 is the number of characters allowed in this '40A' field. If your rootname is longer than 40,\n",
    "    #     you will need to increase this\n",
    "    c1 = fits.Column(name='MEMNAME', array=np.array(rootnames), format='40A') \n",
    "    c2 = fits.Column(name='MEMTYPE', array=np.array(types), format='14A')\n",
    "    c3 = fits.Column(name='MEMPRSNT', format='L', array=included)\n",
    "    t = fits.BinTableHDU.from_columns([c1, c2, c3])\n",
    "    \n",
    "    # Writing the fits table\n",
    "    t.writeto(asn_root.lower()+'_asn.fits', overwrite=True)\n",
    "\n",
    "    print('Saved: {}'.format(asn_root.lower()+'_asn.fits' ), \"in the cwd directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the contents of this new association file below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read(\"filtered_asn.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* that in the \"MEMNAME\" column, we define the location of the corrtag file we wish to process in terms of a local path (the path from the current working directory from which we run our code to the file in the `intermediate` directory.) It is also possible to simply give the \"MEMNAME\" column the file's rootname (i.e `filtered_corrtag_a`) as long as this file and the association files are in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, we make an association file for the unfiltered data files:\n",
    "\n",
    "for f in file_locations_a:\n",
    "    # Adding the file details to the association table\n",
    "    rootnames = [f]  # MEMNAME\n",
    "    types = [type_dict[fits.getval(f, 'EXPTYPE')]] # MEMTYPE\n",
    "    included = [True] # MEMPRSNT\n",
    "    \n",
    "    # Adding the ASN details to the end of the association table\n",
    "    # the rootname needs to be the full name, not just the rootname\n",
    "    asn_root = os.path.basename(f.split('corrtag')[0][:-1]) # removing the extra \"_\" at end of string\n",
    "    rootnames.append(asn_root.upper()) # MEMNAME\n",
    "    types.append('PROD-FP') # MEMTYPE\n",
    "    included.append(True) # MEMPRSNT\n",
    "    \n",
    "    # Putting together the fits table\n",
    "    c1 = fits.Column(name='MEMNAME', array=np.array(rootnames), format='40A') \n",
    "    c2 = fits.Column(name='MEMTYPE', array=np.array(types), format='14A')\n",
    "    c3 = fits.Column(name='MEMPRSNT', format='L', array=included)\n",
    "    t = fits.BinTableHDU.from_columns([c1, c2, c3])\n",
    "    \n",
    "    # Writing the fits table\n",
    "    t.writeto(asn_root.lower()+'_asn.fits', overwrite=True)\n",
    "\n",
    "    print('Saved: {}'.format(asn_root.lower()+'_asn.fits' ), \"in the current working directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see the contents of this association file below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read(f\"{filename_root}_asn.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = CalCOSF></a>\n",
    "## 1.3. Running the `CalCOS` pipeline\n",
    "\n",
    "Now we need to reduce the data using the `CalCOS` pipeline. If you have not already checked out our tutorial on [Running the `CalCOS` pipeline](), it contains vital information and is *highly recommended*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<font size=\"4 \">[If you have not yet downloaded the files as in Section 3 of \"Setup.ipynb\", click to skip this cell](#skipcellF)</font>\n",
    "\n",
    "<font size=\"4 \">If you ran <a href=\"https://github.com/spacetelescope/notebooks/blob/master/notebooks/COS/Setup/Setup.ipynb\">Section 3 of our Notebook on \"Setting up your environment\"</a>, you likely do not need to download more reference files. </font>\n",
    "\n",
    "You can instead simply point to the reference files you downloaded, using the `crds bestrefs` command, as shown in the following three steps. Run these steps from your command line *if and only if* you already have the reference files in a local cache.\n",
    "\n",
    "**1**. The following sets environment variable for crds to look for the reference data online:\n",
    "\n",
    "```$ export CRDS_SERVER_URL=https://hst-crds.stsci.edu``` \n",
    "\n",
    "**2**. The following tells crds where to save the files it downloads - set this to the directory where you saved the crds_cache as in [Section 3 of our Notebook on \"Setup\"](https://github.com/spacetelescope/notebooks/blob/master/notebooks/COS/Setup/Setup.ipynb):\n",
    "\n",
    "```$ export CRDS_PATH=${HOME}/crds_cache```\n",
    "\n",
    "**3**. The following will update the data files you downloaded so that they will be processed with the reference files you previously downloaded. \n",
    "\n",
    "> *Note that these reference files are continually updated and for documentation and to find the newest reference files, see the [CRDS website](https://hst-crds.stsci.edu).*\n",
    "\n",
    "```$ crds bestrefs --files data/*.fits --update-bestrefs --new-context '<the imap or pmap file you used to download the reference files>'```\n",
    "\n",
    "\n",
    "\n",
    "Assuming everything ran successfully, you can now [skip the next several cells to running the pipeline](#runC).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=skipcellF></a>\n",
    "\n",
    "<font size=\"4\"> If you have not yet downloaded the reference files, you will need to do so, as shown below:</font>\n",
    "    \n",
    "Unless we are connected to the STScI network, or already have the reference files on our machine, we will need to download the reference files and tell the pipeline where to look for the flat files, bad-pixel files, etc.\n",
    "\n",
    "<font size=\"4 \"> Caution!</font>\n",
    "\n",
    "<img src=figures/warning.png width =\"60\" title=\"CAUTION!\"> \n",
    "\n",
    "**The process in the following two cells can take a long time and strain network resources!** If you have already downloaded *up-to-date* COS reference files, avoid doing so again.\n",
    "    \n",
    "Instead, keep these crds files in an accessible location, and point an environment variable `lref` to this directory. For instance, if your lref files are on your username's home directory, in a subdirectory called `crds_cache`, give Jupyter the following command then skip the next 2 cells:\n",
    "\n",
    "```%env lref /Users/<your username>/crds_cache/references/hst/cos/```\n",
    "\n",
    "**Again, only run the following two cells if you have not downloaded these files before:**\n",
    "In the next two cells, we will setup an environment of reference files, download the files, and save the output of the crds download process in a log file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "%env lref ./data/reference/references/hst/cos/\n",
    "%env CRDS_SERVER_URL https://hst-crds.stsci.edu\n",
    "%env CRDS_PATH ./data/reference/\n",
    "\n",
    "# The next line depends on your context and pmap file \n",
    "# You can find the latest (\"Operational\") pmap file at https://hst-crds.stsci.edu\n",
    "!crds bestrefs --files **/*corrtag*.fits  --sync-references=2 --update-bestrefs --new-context 'hst_0940.pmap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(outputdir/'crds_output_1.txt'), 'w') as f: # This file now contains the output of the last cell\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=runC></a>\n",
    "\n",
    "<font size=\"4 \"> <b>And now, to run the pipeline itself:</b></font>\n",
    "\n",
    "Again, *because we wish to compare against the unfiltered data*, we must run the pipeline twice:\n",
    "1. For these new **filtered** files\n",
    "2. For the original **unfiltered** files, so that we can compare and see the effect of filtering. \n",
    "\n",
    "*You can ignore any `AstropyDeprecationWarning`s that pop up*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# First, run with the \"filtered\" data with only time-tag datapoints allowed by the filter\n",
    "calcos.calcos('./filtered_asn.fits', outdir=str(outputdir / \"filtered_data_outs\"), verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# Now, run CalCOS with the initial \"full\" data - with all time-tag datapoints\n",
    "calcos.calcos(f'./{filename_root}_asn.fits', outdir=str(outputdir / \"full_data_outs\"), verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = compF></a>\n",
    "# 2. Comparing the filtered and unfiltered data\n",
    "**Excellent!** We're essentially done at this point. \n",
    "\n",
    "Let's read in both of the processed spectra (the `x1dsum` files) and plot the spectra against one another.\n",
    "\n",
    "*You can ignore any `UnitsWarning`s that pop up about formatting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the wvln, flux, flux error of the *UNfiltered* spectrum file\n",
    "unfilt_tab = Table.read(\"./output/full_data_outs/\"+filename_root+\"_x1dsum.fits\")['WAVELENGTH','FLUX', 'ERROR']\n",
    "# Read in the wvln, flux, flux error of the *filtered* spectrum file\n",
    "filt_tab = Table.read(\"./output/filtered_data_outs/filtered_x1dsum.fits\")['WAVELENGTH','FLUX', 'ERROR']\n",
    "\n",
    "combo_dict_f = {'WAVELENGTH':[], 'FLUX':[], 'ERROR':[]} # Convert to dict and combine segments\n",
    "combo_dict_u = {'WAVELENGTH':[], 'FLUX':[], 'ERROR':[]}\n",
    "\n",
    "for row in filt_tab[::-1]: # reverse segments FUVA and B for filtered data\n",
    "    for key in row.colnames:\n",
    "        combo_dict_f[key]+=(list(row[key]))\n",
    "        \n",
    "for row in unfilt_tab[::-1]: # reverse segments FUVA and B for UNfiltered data\n",
    "    for key in row.colnames:\n",
    "        combo_dict_u[key]+=(list(row[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure:\n",
    "fig = plt.figure(figsize=(15, 15)) \n",
    "gs = gridspec.GridSpec(3,1, height_ratios=[3, 1,1]) \n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax2 = plt.subplot(gs[2])\n",
    "\n",
    "# Plot the data in 3 subplots\n",
    "ax0.scatter(combo_dict_u[\"WAVELENGTH\"], combo_dict_u[\"FLUX\"], s = 2, alpha = 1, c = 'C1',label = \"Unfiltered\")\n",
    "ax0.scatter(combo_dict_f[\"WAVELENGTH\"], combo_dict_f[\"FLUX\"], s = 2, alpha = 1, c = 'C2', label = \"Filtered to SUN_ALT<0\")\n",
    "\n",
    "ax1.scatter(combo_dict_u[\"WAVELENGTH\"], combo_dict_u[\"FLUX\"], s = 2, alpha = 1, c = 'C1',label = \"Unfiltered\")\n",
    "ax2.scatter(combo_dict_f[\"WAVELENGTH\"], combo_dict_f[\"FLUX\"], s = 2, alpha = 1, c = 'C2', label = \"Filtered to SUN_ALT<0\")\n",
    "\n",
    "# Format the figure\n",
    "ax0.set_xticks([])\n",
    "ax1.set_xticks([])\n",
    "\n",
    "ax0.legend(fontsize=20)\n",
    "ax1.legend(fontsize=20)\n",
    "ax2.legend(fontsize=20)\n",
    "ax0.set_ylim(-2E-14, 6.5E-13)\n",
    "ax1.set_ylim(-2E-14, 6.5E-13)\n",
    "ax2.set_ylim(ax1.get_ylim())\n",
    "\n",
    "ax0.set_title(\"Fig 2.1\\nSpectra Filtered and Unfiltered\\nby Solar altitude (`SUN_ALT`)\", size = 35)\n",
    "ax0.set_ylabel(\"Flux [$\\dfrac{erg}{cm\\ s\\ \\AA}$]\", \n",
    "               fontsize=30, y=0.0, horizontalalignment='center')\n",
    "ax2.set_xlabel(\"Wavelength [$\\AA$]\", fontsize=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(str(plotsdir / 'compare_spectra_sunalt.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the filtered spectrum largely follows the unfiltered spectrum; however, we significantly reduce the Lyman-alpha light from airglow around 1215 Å.**\n",
    "\n",
    "Because we filter out many of the datapoints used to calculate the spectrum, we can see a significant reduction in precision in flux space (visible as a **banding** in the y-axis). This can come about because with few datapoints, the bands represent wavelengths which received (0,1,2...n) discrete photons. The banding can sometimes be more pronounced at longer, redder wavelengths. \n",
    "\n",
    "**Below, let's make one final plot: a segment of the spectrum around Lyman-alpha (\\~1215 Å) showing the errors in flux:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0) = plt.subplots(nrows=1, ncols=1, figsize=(15,8))\n",
    "\n",
    "ax0.errorbar(combo_dict_u[\"WAVELENGTH\"], combo_dict_u[\"FLUX\"], combo_dict_u[\"ERROR\"], \n",
    "             linestyle = '', markersize = 2, alpha = 1, c = 'C1',label = \"Unfiltered\")\n",
    "ax0.errorbar(combo_dict_f[\"WAVELENGTH\"], combo_dict_f[\"FLUX\"], combo_dict_f[\"ERROR\"], \n",
    "             linestyle = '', markersize = 2, alpha = 1, c = 'C2', label = \"Filtered to SUN_ALT<0\")\n",
    "\n",
    "ax0.legend(fontsize = 20)\n",
    "ax0.set_xlabel(\"Wavelength [$\\AA$]\", fontsize = 30)\n",
    "ax0.set_ylabel(\"Flux [$\\dfrac{erg}{cm\\ s\\ \\AA}$]\", fontsize = 30, horizontalalignment='center')\n",
    "ax0.set_title(\"Fig 2.2\\nSpectra with Errors of Filtered and Unfiltered by `SUN_ALT`\", size = 30)\n",
    "ax0.set_xlim(1213,1218)\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(plotsdir / 'ebar_compare_spectra_sunalt.png'), dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**With substantially fewer datapoints, our filtered dataset has larger errors.**\n",
    "\n",
    "We can, however, understand why we might want to filter by this, or another `sun_alt` filter. For instance, if most of your exposure was taken at night, but the last 10% was taken after the sun had risen and induced an atmospheric line which interferes with your data, it would often be necessary to filter out this last 10% of the exposure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In conclusion\n",
    "* We have learned how to use `costools.timefilter` to separate data taken during HST's day from data taken during the observatory's night\n",
    "* We have discussed the benefits of using this method to remove light from Earth's upper atmosphere, as well as the associated loss in signal-to-noise ratio that comes from shortening the effective exposure time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You finished this Notebook!\n",
    "<font size=\"5\">There are more COS data walkthrough Notebooks on different topics. You can find them <a href=\"https://spacetelescope.github.io/COS-Notebooks/\">here</a>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## About this Notebook\n",
    "**Author:** Nat Kerman <nkerman@stsci.edu>\n",
    "\n",
    "**Contributors:** Elaine Mae Frazer\n",
    "\n",
    "**Updated On:** 2021-09-03\n",
    "\n",
    "> *This tutorial was generated to be in compliance with the [STScI style guides](https://github.com/spacetelescope/style-guides) and would like to cite the [Jupyter guide](https://github.com/spacetelescope/style-guides/blob/master/templates/example_notebook.ipynb) in particular.*\n",
    "\n",
    "## Citations\n",
    "\n",
    "If you use `astropy`, `matplotlib`, `astroquery`, or `numpy` for published research, please cite the\n",
    "authors. Follow these links for more information about citations:\n",
    "\n",
    "* [Citing `astropy`/`numpy`/`matplotlib`](https://www.scipy.org/citing.html)\n",
    "* [Citing `astroquery`](https://astroquery.readthedocs.io/en/latest/)\n",
    "\n",
    "---\n",
    "\n",
    "[Top of Page](#topF)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> \n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "85f11517647c923e8a5c2cfc30b3394d5584448607201201c64f3e88261ee333"
  },
  "kernelspec": {
   "display_name": "Python [conda env:fs2]",
   "language": "python",
   "name": "conda-env-fs2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
