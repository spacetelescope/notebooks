{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topAF\"></a>\n",
    "\n",
    "# Modifying or Creating an Association File\n",
    "\n",
    "# Learning Goals\n",
    "<font size=\"4\"> This Notebook is designed to walk the user (<em>you</em>) through: <b>Creating or altering the association (<tt>asn</tt>) file used by the Cosmic Origins Spectrograph (<em>COS</em>) pipeline to determine which data to process:</b> </font><br>\n",
    "    \n",
    "**1. [Examining an association file](#examAF)**\n",
    "\n",
    "**2. [Editing an existing association file](#editAF)**\n",
    "\n",
    "\\- 2.1. [Removing an exposure](#subAF)\n",
    "    \n",
    "\\- 2.2. [Adding an exposure](#addAF)\n",
    "    \n",
    "**3. [Creating an entirely new association file](#newAF)**\n",
    "\n",
    "\\- 3.1. [Simplest method](#simpleAF)\n",
    "    \n",
    "\\- 3.2. [With fits header metadata](#metaAF)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "**The Cosmic Origins Spectrograph ([*COS*](https://www.nasa.gov/content/hubble-space-telescope-cosmic-origins-spectrograph)) is an ultraviolet spectrograph on-board the Hubble Space Telescope ([*HST*](https://www.stsci.edu/hst/about)) with capabilities in the near ultraviolet (*NUV*) and far ultraviolet (*FUV*).**\n",
    "\n",
    "**This tutorial aims to prepare you to alter the association file used by the `CalCOS` pipeline.** Association files are `fits` files containing a binary table extension, which lists science and calibration exposures which the pipeline will process together.\n",
    "\n",
    "- For an in-depth manual to working with COS data and a discussion of caveats and user tips, see the [COS Data Handbook](https://hst-docs.stsci.edu/display/COSDHB/).\n",
    "- For a detailed overview of the COS instrument, see the [COS Instrument Handbook](https://hst-docs.stsci.edu/display/COSIHB/).\n",
    "\n",
    "<font size=\"4\">We'll demonstrate creating an <tt>asn</tt> file in two ways: First, we'll demonstrate <b>editing an existing <tt>asn</tt> file</b> to add or remove an exposure. Second, we'll show how to <b>create an entirely new <tt>asn</tt> file</b>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will import the following packages:\n",
    "\n",
    "- `numpy` to handle array functions\n",
    "- `astropy.io fits` and `astropy.table Table` for accessing FITS files\n",
    "- `glob`, `os`, and `shutil` for working with system files\n",
    "  - `glob` helps us search for filenames\n",
    "  - `os` and `shutil` for moving files and deleting folders, respectively\n",
    "- `astroquery.mast Mast` and `Observations` for finding and downloading data from the [MAST](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html) archive\n",
    "- `datetime` for updating fits headers with today's date\n",
    "- `pathlib Path` for managing system paths\n",
    "\n",
    "If you have an existing astroconda environment, it may or may not already have the necessary packages to run this Notebook. To create a Python environment capable of running all the data analyses in these COS Notebooks, please see Section 1 of our Notebook tutorial on [setting up an environment](https://github.com/spacetelescope/notebooks/blob/master/notebooks/COS/Setup/Setup.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for: array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# Import for: reading fits files\n",
    "from astropy.io import fits                                            \n",
    "from astropy.table import Table\n",
    "\n",
    "# Import for: system files\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Import for: downloading the data\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# Import for: changing modification date in a fits header\n",
    "import datetime\n",
    "\n",
    "#Import for: working with system paths\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will also define a few directories we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will be important directories for the Notebook\n",
    "\n",
    "datadir = Path('./data/')\n",
    "outputdir = Path('./output/')\n",
    "plotsdir = Path('./output/plots/')\n",
    "\n",
    "# Make the directories if they don't already exist\n",
    "datadir.mkdir(exist_ok=True), outputdir.mkdir(exist_ok=True), plotsdir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we will need to download the data we wish to filter and analyze\n",
    "We choose the exposures with the association obs_ids: `ldif01010` and `ldif02010` because we happen to know that some of the exposures in these groups failed, which gives us a real-world use case for editing an association file. Both `ldif01010` and `ldif02010` are far-ultraviolet (FUV) datasets on the quasi-stellar object (QSO) [PDS 456](https://doi.org/10.1051/0004-6361/201935524).\n",
    "\n",
    "For more information on downloading COS data, see our [Notebook tutorial on downloading COS data](https://github.com/spacetelescope/notebooks/blob/master/notebooks/COS/DataDl/DataDl.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Observations.get_product_list(Observations.query_criteria(obs_id='ldif0*10')) # search for the correct obs_ids and get the product list \n",
    "fpl = Observations.filter_products(pl,\n",
    "                                   productSubGroupDescription=['RAWTAG_A', 'RAWTAG_B','ASN']) # filter to rawtag and asn files in the product list\n",
    "\n",
    "Observations.download_products(fpl, download_dir=str(datadir)) # Download these chosen products\n",
    "for gfile in glob.glob(\"**/ldif*/*.fits\", recursive=True): # Move all fits files in this set to the base data directory\n",
    "    os.rename(gfile,datadir / os.path.basename(gfile))\n",
    "shutil.rmtree(datadir / 'mastDownload') # Delete the now-empty, nested mastDownload directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = examAF></a>\n",
    "# 1. Examining an association file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we downloaded two association files and their rawtag data files. We will begin by searching for the association files and reading one of them (`LDIF01010`). We could just as easily pick `ldif02010`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asnfiles = glob.glob(\"**/*ldif*asn*\", recursive=True) # There will be two (ldif01010_asn.fits and ldif02010_asn.fits)\n",
    "asnfile = asnfiles[0] # We want to work primarily with ldif01010_asn.fits\n",
    "\n",
    "asn_contents = Table.read(asnfile) # Gets the contents of the asn file\n",
    "asn_contents # Display these contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the association file has five rows: four exposures denoted with the `MEMTYPE` = `EXP-FP`, and a product with `MEMTYPE` = `PROD-FP`.\n",
    "\n",
    "In the cell below, we examine a bit about each of the exposures as a diagnostic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for memname, memtype in zip(asn_contents['MEMNAME'], asn_contents[\"MEMTYPE\"]): # Cycles through each file in asn table\n",
    "    memname = memname.lower() # Find file names in lower case letters\n",
    "    if memtype == 'EXP-FP': # We only want to look at the exposure files\n",
    "        rt_a = (glob.glob(f\"**/*{memname}*rawtag_a*\", recursive=True))[0] # Find the actual filepath of the memname for rawtag_a and rawtag_b\n",
    "        rt_b = (glob.glob(f\"**/*{memname}*rawtag_b*\", recursive=True))[0]\n",
    "\n",
    "        # Now print all these diagnostics:\n",
    "        print(f\"Association {(fits.getheader(rt_a))['ASN_ID']} has {memtype} exposure {memname.upper()} with \\\n",
    "exposure time {(fits.getheader(rt_a, ext=1))['EXPTIME']} seconds at cenwave {(fits.getheader(rt_a, ext=0))['CENWAVE']} \\\n",
    "Ã… and FP-POS {(fits.getheader(rt_a, ext=0))['FPPOS']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We notice that something seems amiss with exposure LDIF01TYQ**:\n",
    "This file has an exposure time of 0.0 seconds - something has gone wrong. In this case, there was a guide star acquisition failure as described on the [data preview page](http://archive.stsci.edu/cgi-bin/mastpreview?mission=hst&dataid=LDIF01010).\n",
    "\n",
    "In the next section, we will correct this lack of data by replacing the bad exposure with an exposure from the other association group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = editAF></a>\n",
    "# 2. Editing an existing association file\n",
    "\n",
    "<a id = subAF></a>\n",
    "## 2.1. Removing an exposure\n",
    "\n",
    "We know that at least one of our exposures - `ldif01tyq` - is not suited for combination into the final product. It has an exposure time of 0.0 seconds, in this case from a guide star acquisition failure. This is a generalizable issue, as you may often know an exposure is \"*bad*\" for many reasons: perhaps it was taken with the shutter closed, or with anomolously high background noise, or any number of reasons we may wish to exclude an exposure from our data. To do this, we will need to alter our existing association file before we re-run `CalCOS`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again see the contents of our main association file below. Note that `True/False` and `1/0` are essentially interchangable in the `MEMPRSNT` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read(asnfiles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the `MEMPRSNT` value to `False` or `0` for our bad exposure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(asnfile, mode='update') as hdulist: # We need to change values with the asnfile opened and in 'update' mode\n",
    "    tbdata = hdulist[1].data # This is where the table data is read into\n",
    "    for expfile in tbdata: # Check if each file is one of the bad ones\n",
    "        if expfile['MEMNAME'] in ['LDIF01TYQ']:\n",
    "            expfile['MEMPRSNT'] = False # If so, set MEMPRSNT to False AKA 0\n",
    "            \n",
    "Table.read(asnfile) # Re-read the table to see the change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = addAF></a>\n",
    "## 2.2. Adding an exposure\n",
    "We removed the failed exposure taken with `FP-POS = 1`. Usually we want to combine one of each of the four [*fixed-pattern noise positions* (`FP-POS`)](https://hst-docs.stsci.edu/cosdhb/chapter-1-cos-overview/1-1-instrument-capabilities-and-design), so let's add the `FP-POS = 1` exposure from the other association group.\n",
    "\n",
    "In the cell below, we determine which exposure from `LDIF02010` was taken with `FP-POS = 1`.\n",
    "- *It does this by looping through the files listed in `LDIF02010`'s association file, and then reading in that file's header to check if its `FPPOS` value equals 1.*\n",
    "- *It also prints some diagnostic information about all of the esxposure files.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_contents_2 = Table.read(asnfiles[1]) # Reads the contents of the 2nd asn file\n",
    "\n",
    "for memname, memtype in zip(asn_contents_2['MEMNAME'], asn_contents_2[\"MEMTYPE\"]): # Loops through each file in asn table for `LDIF02010`\n",
    "    memname = memname.lower() # Convert file names to lower case letters, as in actual filenames\n",
    "    if memtype == 'EXP-FP': # We only want to look at the exposure files\n",
    "        rt_a = (glob.glob(f\"**/*{memname}*rawtag_a*\", recursive=True))[0] # Search for the actual filepath of the memname for rawtag_a \n",
    "        rt_b = (glob.glob(f\"**/*{memname}*rawtag_b*\", recursive=True))[0] # Search for the actual filepath of the memname for rawtag_b \n",
    "        # Now print all these diagnostics:\n",
    "        print(f\"Association {(fits.getheader(rt_a))['ASN_ID']} has {memtype} exposure {memname.upper()} with \\\n",
    "exptime {(fits.getheader(rt_a, ext=1))['EXPTIME']} seconds at cenwave {(fits.getheader(rt_a, ext=0))['CENWAVE']} Ã… and FP-POS {(fits.getheader(rt_a, ext=0))['FPPOS']}.\")\n",
    "\n",
    "        if (fits.getheader(rt_a, ext=0))['FPPOS'] == 1:\n",
    "            print(f\"^^^ The one above this has the FP-POS we are looking for ({memname.upper()})^^^\\n\")\n",
    "            asn2_fppos1_name = memname.upper() # Save the right file basename in a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a slightly different procedure to add a new exposure to the list rather than remove one. \n",
    "\n",
    "Here we want to read the table in the fits association file into an `astropy` Table. We can then add a row into the right spot, filling it with the new file's `MEMNAME`, `MEMTYPE`, and `MEMPRSNT`. Finally, we have to save this table into the existing fits association file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_orig_table = Table.read(asnfile) # Read in original data from the file\n",
    "asn_orig_table.insert_row(len(asn_orig_table)- 1 , [asn2_fppos1_name,'EXP-FP',1]) # Add a row with the right name after all the original EXP-FP's\n",
    "new_table = fits.BinTableHDU(asn_orig_table) # Turn this into a fits Binary Table HDU \n",
    "\n",
    "with fits.open(asnfile, mode='update') as hdulist: # We need to change data with the asnfile opened and in 'update' mode\n",
    "    hdulist[1].data = new_table.data  # Change the orig file's data to the new table data we made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see there is a new row with our exposure from the other `asn` file group: `LDIF02NMQ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read(asnfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4 \"><b>Excellent.</b> In the next section we will create a new association file from scratch.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = newAF></a>\n",
    "# 3. Creating an entirely new association file\n",
    "\n",
    "For the sake of demonstration, we will generate a new association file with four exposure members: even-numbered `FP-POS` (2,4) from the first original association (`LDIF01010`), and odd-numbered `FP-POS` (1,3) from from the second original association (`LDIF02010`).\n",
    "\n",
    "From section 2, we see that this corresponds to :\n",
    "\n",
    "|Name|Original asn|FP-POS|\n",
    "|----|------------|------|\n",
    "|LDIF02010|LDIF02NMQ|1|\n",
    "|LDIF01010|LDIF01U0Q|2|\n",
    "|LDIF02010|LDIF02NUQ|3|\n",
    "|LDIF01010|LDIF01U4Q|4|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = simpleAF></a>\n",
    "## 3.1. Simplest method\n",
    "Below, we manually build up an association file from the three necessary columns:\n",
    "1. `MEMNAME`\n",
    "2. `MEMTYPE`\n",
    "3. `MEMPRSNT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the exposure file details to the association table\n",
    "new_asn_memnames = ['LDIF02NMQ','LDIF01U0Q','LDIF02NUQ','LDIF01U4Q'] # MEMNAME\n",
    "types = ['EXP-FP', 'EXP-FP', 'EXP-FP', 'EXP-FP'] # MEMTYPE\n",
    "included = [True, True, True, True] # MEMPRSNT\n",
    "\n",
    "# Adding the ASN details to the end of the association table\n",
    "new_asn_memnames.append('ldifcombo'.upper()) # MEMNAME column\n",
    "types.append('PROD-FP') # MEMTYPE column\n",
    "included.append(True) # MEMPRSNT column\n",
    "\n",
    "# Putting together the fits table\n",
    "#   40 is the number of characters allowed in this field with the MEMNAME format = 40A. \n",
    "#    If your rootname is longer than 40, you will need to increase this\n",
    "c1 = fits.Column(name='MEMNAME', array=np.array(new_asn_memnames), format='40A') \n",
    "c2 = fits.Column(name='MEMTYPE', array=np.array(types), format='14A')\n",
    "c3 = fits.Column(name='MEMPRSNT', format='L', array=included)\n",
    "asn_table = fits.BinTableHDU.from_columns([c1, c2, c3])\n",
    "\n",
    "# Writing the fits table\n",
    "asn_table.writeto(outputdir / 'ldifcombo_asn.fits', overwrite=True)\n",
    "\n",
    "print('Saved: '+ 'ldifcombo_asn.fits'+ f\" in the output directory: {outputdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examining the file we have created:**\n",
    "\n",
    "We see that the data looks correct - exactly the table we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read(outputdir / 'ldifcombo_asn.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However, the 0th and 1st fits headers no longer contain useful information about the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits.getheader(outputdir / 'ldifcombo_asn.fits', ext=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits.getheader(outputdir / 'ldifcombo_asn.fits', ext=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = metaAF></a>\n",
    "## 3.2. With fits header metadata\n",
    "\n",
    "**We can instead build up a new file with our old file's fits header, and alter it to reflect our changes.**\n",
    "\n",
    "We first build a new association file, a piecewise combination of our original file's headers and our new table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(asnfile, mode='readonly') as hdulist: # Open up the old asn file\n",
    "    hdulist.info() # Shows the first hdu is empty except for the header we want\n",
    "    hdu0 = hdulist[0] # We want to directly copy over the old 0th header/data-unit AKA \"hdu\":\n",
    "                        # essentially a section of data and its associated metadata, called a \"header\"\n",
    "                        # see https://fits.gsfc.nasa.gov/fits_primer.html for info on fits structures\n",
    "    d0 = hdulist[0].data # gather the data from the header/data unit to allow the readout\n",
    "    h1 = hdulist[1].header # gather the header from the 1st header/data unit to copy to our new file\n",
    "    \n",
    "hdu1 = fits.BinTableHDU.from_columns([c1, c2, c3], header=h1) # Put together new 1st hdu from old header and new data\n",
    "\n",
    "new_HDUlist = fits.HDUList([hdu0,hdu1]) # New HDUList from old HDU 0 and new combined HDU 1\n",
    "new_HDUlist.writeto(outputdir / 'ldifcombo_2_asn.fits', overwrite=True) # Write this out to a new file\n",
    "new_asnfile = outputdir / 'ldifcombo_2_asn.fits' # Path to this new file\n",
    "print('\\nSaved: '+ 'ldifcombo_2_asn.fits'+ f\"in the output directory: {outputdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we edit the relevant values in our fits headers that are different from the original.**\n",
    "\n",
    "*Note: It is possible that a generic fits file may have different values you may wish to change. It is highly recommended to examine your fits headers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.date.today() # Find today's date\n",
    "# Below, make a dict of what header values we want to change, corresponding to [new value, extension the value lives in, 2nd extension if applies]\n",
    "keys_to_change = {'DATE':[f'{date.year}-{date.month}-{date.day}',0], 'FILENAME':['ldifcombo_2_asn.fits',0],\n",
    "                      'ROOTNAME':['ldifcombo_2',0,1], 'ASN_ID':['ldifcombo_2',0], 'ASN_TAB':['ldifcombo_2_asn.fits',0], 'ASN_PROD':['False',0],\n",
    "                     'EXTVER':[2,1], 'EXPNAME':['ldifcombo_2',1]}\n",
    "# Actually change the values below (verbosely):\n",
    "for keyval in keys_to_change.items():\n",
    "    print(f\"Editing {keyval[0]} in Extension {keyval[1][1]}\")\n",
    "    fits.setval(filename=new_asnfile, keyword=keyval[0], value=keyval[1][0], ext=keyval[1][1])\n",
    "    # Below is necessary as some keys are repeated in both headers ('ROOTNAME')\n",
    "    if len(keyval[1])>2:\n",
    "        print(f\"Editing {keyval[0]} in Extension {keyval[1][2]}\")\n",
    "        fits.setval(filename=new_asnfile, keyword=keyval[0], value= keyval[1][0], ext=keyval[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4 \"><b>And now we have created our new association file.</b> The file is now ready to be used in the <code>CalCOS</code> pipeline!</font>\n",
    "    \n",
    "If you're interested in testing your file by running it through the `CalCOS` pipeline, you may wish to run the `test_asn.py file` included in this subdirectory of the GitHub repository. i.e. from the command line: \n",
    "\n",
    "```bash\n",
    "$ python test_asn.py\n",
    "```\n",
    "*Note* that you must first...\n",
    "1. Have created the file by running this Notebook\n",
    "2. Alter line 21 of `test_asn.py` to set the lref directory to wherever you have your cache of CRDS reference files (see our [Setup Notebook](https://github.com/spacetelescope/notebooks/blob/master/notebooks/COS/Setup/Setup.ipynb)).\n",
    "\n",
    "If the test runs successfully, it will create a plot in the subdirectory `./output/plots/` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You finished this Notebook!\n",
    "<font size=\"5\">There are more COS data walkthrough Notebooks on different topics. You can find them <a href=\"https://spacetelescope.github.io/COS-Notebooks/\">here</a>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## About this Notebook\n",
    "**Author:** Nat Kerman: <nkerman@stsci.edu>\n",
    "\n",
    "**Contributors:** Elaine Mae Frazer\n",
    "\n",
    "**Updated On:** 2021-7-06\n",
    "\n",
    "> *This tutorial was generated to be in compliance with the [STScI style guides](https://github.com/spacetelescope/style-guides) and would like to cite the [Jupyter guide](https://github.com/spacetelescope/style-guides/blob/master/templates/example_notebook.ipynb) in particular.*\n",
    "\n",
    "## Citations\n",
    "\n",
    "If you use `astropy`, `matplotlib`, `astroquery`, or `numpy` for published research, please cite the\n",
    "authors. Follow these links for more information about citations:\n",
    "\n",
    "* [Citing `astropy`/`numpy`/`matplotlib`](https://www.scipy.org/citing.html)\n",
    "* [Citing `astroquery`](https://astroquery.readthedocs.io/en/latest/)\n",
    "\n",
    "---\n",
    "\n",
    "[Top of Page](#topAF)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> \n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
