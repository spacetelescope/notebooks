{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topF\"></a>\n",
    "\n",
    "# Filtering out COS Data taken during the Day or Night\n",
    "\n",
    "# Learning Goals\n",
    "### This Notebook is designed to walk the user (*you*) through: **Filtering Cosmic Origins Spectrograph (*COS*) `TIME-TAG` data taken during the day from data taken during the night**\n",
    "   #### 1. [**Processing a spectrum from a filtered dataset**](#procF)\n",
    "   ##### - 1.1. [Filtering the `TIME-TAG` data](#filtF)\n",
    "   ##### - 1.2. [Creating a new association file](#asnF)\n",
    "   ##### - 1.3. [Running the `calcos` pipeline on the filtered dataset](#calcosF)\n",
    "   #### 2. [**Comparing the filtered and unfiltered data**](#compF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "#### The Cosmic Origins Spectrograph ([*COS*](https://www.nasa.gov/content/hubble-space-telescope-cosmic-origins-spectrograph)) is an ultraviolet spectrograph on-board the Hubble Space Telescope([*HST*](https://www.stsci.edu/hst/about)) with capabilities in the near ultraviolet (*NUV*) and far ultraviolet (*FUV*). \n",
    "\n",
    "#### This tutorial aims to prepare you to work with the COS data of your choice by walking you through filtering `TIME-TAG` datapoints obtained by COS. \n",
    "\n",
    "In particular, this tutorial will walk you through separating datapoints obtained during the Hubble Space Telescope's \"night\" - when the sun is below the geometric horizon from the observatory's point of view - from datapoints taken during the observatory's \"day\" - when the sun is above this horizon. This type of data separation is possible with the [`TIME-TAG` data](https://hst-docs.stsci.edu/cosdhb/chapter-1-cos-overview/1-1-instrument-capabilities-and-design) obtained by the COS photon-counting detectors, because each individual encounter with a photon is recorded with its own metadata such as the time of the encounter, and the physical position of Hubble at that time.\n",
    "\n",
    "- For an in-depth manual to working with COS data and a discussion of caveats and user tips, see the [COS Data Handbook](https://hst-docs.stsci.edu/display/COSDHB/).\n",
    "- For a detailed overview of the COS instrument, see the [COS Instrument Handbook](https://hst-docs.stsci.edu/display/COSIHB/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will import the following packages:\n",
    "\n",
    "- costools timefilter to select `TIME-TAG` datapoints by their metadata parameters\n",
    "- calcos to re-process the data\n",
    "- numpy to handle array functions\n",
    "- astropy.io fits and astropy.table Table for accessing FITS files\n",
    "- glob and os for working with system files\n",
    "- matplotlib.pyplot and gridspec for plotting data\n",
    "- astroquery.mast Mast and Observations for finding and downloading data from the [MAST](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html) archive\n",
    "\n",
    "These python packages, including `calcos` and `costools` are installed standard with the the STScI conda distribution. For more information, see our notebook tutorial on [setting up an environment](https://github.com/spacetelescope/COS-Notebooks/blob/master/Setup/Setup.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filtering i.e. by sun altitude\n",
    "from costools import timefilter\n",
    "# for processing cos data\n",
    "import calcos \n",
    "# for array manipulation\n",
    "import numpy as np\n",
    "# for reading fits files\n",
    "from astropy.io import fits                                            \n",
    "from astropy.table import Table\n",
    "# for system files\n",
    "import glob\n",
    "import os\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "# for downloading the data\n",
    "from astroquery.mast import Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will also define a few directories we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/'\n",
    "intermediate_dir = './intermediate/'\n",
    "output_dir = './output/'\n",
    "plots_dir = output_dir + 'plots/'\n",
    "# Make the directories in case they don't exist\n",
    "!mkdir ./data\n",
    "!mkdir ./intermediate\n",
    "!mkdir ./output \n",
    "!mkdir ./output/plots/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we will need to download the data we wish to filter and analyze\n",
    "We somewhat arbitrarily choose the exposure with obs_id: `lb3q01meq`, because we happen to know it contains data taken both in the observatory's night and day. For more information on downloading COS data, see our [notebook tutorial on downloading COS data](https://github.com/spacetelescope/COS-Notebooks/blob/master/DataDL/DataDl.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_root = 'lb3q01meq'\n",
    "\n",
    "pl = Observations.get_product_list(Observations.query_criteria(proposal_id = '11533', obs_id = 'lb3q01*'))\n",
    "pl_mask = ((pl['productSubGroupDescription'] == \"CORRTAG_A\") | (pl['productSubGroupDescription'] == \"CORRTAG_B\")) & (pl['obs_id'] == filename_root)\n",
    "Observations.download_products(pl[pl_mask], download_dir = './data')\n",
    "\n",
    "!mv ./data/**/*.fits ./data # grab the data out of its long convoluted filepath\n",
    "# NOTE - slightly dangerous if you may have any non-unique filenames - if you have lots of data, probably don't your aggregate data.\n",
    "\n",
    "file_locations_a = glob.glob('./**/*corrtag_a.fits', recursive=True)\n",
    "file_locations_b = glob.glob('./**/*corrtag_b.fits', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = procF></a>\n",
    "# 1. Processing a spectrum from a filtered dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = filtF></a>\n",
    "## 1.1. Filtering the `TIME-TAG` data\n",
    "The `costools` package contains the `TimelineFilter` class, which - upon instantiation - filters by the parameters you give it. In other words, you don't have to run any functions or methods aside from instantiating the class. This is done by passing the following parameters to `timefilter.TimelineFilter`:\n",
    "- input `time-tag` filepath (string)\n",
    "- output `time-tag` filepath (string)\n",
    "- filter (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for afile, bfile in zip(file_locations_a,file_locations_b): # This must be repeated for all exposures\n",
    "    timefilter.TimelineFilter(input=afile, output=intermediate_dir+\"filtered_corrtag_a.fits\", \n",
    "                              filter = \"sun_alt > 0.\", verbose=True)\n",
    "    timefilter.TimelineFilter(bfile, intermediate_dir+\"filtered_corrtag_b.fits\",\n",
    "                                  \"sun_alt > 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More filtering parameters can be found in the [Section 5.4.2 of the COS Data Handbook: Filtering Time-Tag Data](https://hst-docs.stsci.edu/cosdhb/chapter-5-cos-data-analysis/5-4-working-with-time-tag-data). \n",
    "\n",
    "*Some, but not all, of the available filters are*:\n",
    "- time\n",
    "- observatory latitude/longitude\n",
    "- dark current rate\n",
    "- any combination of filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = asnF></a>\n",
    "## 1.2. Creating a new association file\n",
    "In order to run the `calcos` pipeline on your newly filtered data, an association (`asn`) file must be made, instructing the pipeline where to look for the filtered `TIME-TAG` data files. In the next cell, we build **two** new association file from scratch:\n",
    "1. For these new **filtered** files\n",
    "2. For the original **unfiltered** files, so that we can compare and see the effect of filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST, for the newly filtered-by-sun_alt files\n",
    "\n",
    "# this properly assigns the type of exposure in a way that CalCOS will recognize\n",
    "type_dict = {'WAVECAL' : 'EXP-AWAVE',\n",
    "             'EXTERNAL/SCI' : 'EXP-FP'}\n",
    "\n",
    "files = glob.glob(intermediate_dir+\"*_corrtag_a.fits\") \n",
    "# These file locations and names will be different depending on your output name \n",
    "# You only need the A segment in the ASN, and calcos will find the associated B segment data for you\n",
    "for f in files:\n",
    "    \n",
    "    # Adding the file details to the association table\n",
    "    rootnames = [f]  # MEMNAME\n",
    "    types = [type_dict[fits.getval(f, 'EXPTYPE')]] # MEMTYPE\n",
    "    included = [True] # MEMPRSNT\n",
    "    \n",
    "    # Adding the ASN details to the end of the association table\n",
    "    # the rootname needs to be the full name, not just the rootname\n",
    "    asn_root = os.path.basename(f.split('corrtag')[0][:-1]) # removing the extra \"_\" at the end of this\n",
    "    rootnames.append(asn_root.upper()) # MEMNAME\n",
    "    types.append('PROD-FP') # MEMTYPE\n",
    "    included.append(True) # MEMPRSNT\n",
    "    \n",
    "    # Putting together the fits table\n",
    "    #   40 is the number of characters allowed in this field. If your rootname is longer than 40, \n",
    "    #     you will need to increase this\n",
    "    c1 = fits.Column(name='MEMNAME', array=np.array(rootnames), format='40A') \n",
    "    c2 = fits.Column(name='MEMTYPE', array=np.array(types), format='14A')\n",
    "    c3 = fits.Column(name='MEMPRSNT', format='L', array=included)\n",
    "    t = fits.BinTableHDU.from_columns([c1, c2, c3])\n",
    "    \n",
    "    # Writing the fits table\n",
    "    t.writeto(asn_root.lower()+'_asn.fits')\n",
    "\n",
    "    print('Saved: {}'.format(asn_root.lower()+'_asn.fits' ), \"in the cwd directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the contents of this association file below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read(\"filtered_asn.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW, we make an association file for the unfiltered data files\n",
    "\n",
    "for f in file_locations_a:\n",
    "    # Adding the file details to the association table\n",
    "    rootnames = [f]  # MEMNAME\n",
    "    types = [type_dict[fits.getval(f, 'EXPTYPE')]] # MEMTYPE\n",
    "    included = [True] # MEMPRSNT\n",
    "    \n",
    "    # Adding the ASN details to the end of the association table\n",
    "    # the rootname needs to be the full name, not just the rootname\n",
    "    asn_root = os.path.basename(f.split('corrtag')[0][:-1]) # removing the extra \"_\" at the end of this\n",
    "    rootnames.append(asn_root.upper()) # MEMNAME\n",
    "    types.append('PROD-FP') # MEMTYPE\n",
    "    included.append(True) # MEMPRSNT\n",
    "    \n",
    "    # Putting together the fits table\n",
    "    c1 = fits.Column(name='MEMNAME', array=np.array(rootnames), format='40A') \n",
    "    c2 = fits.Column(name='MEMTYPE', array=np.array(types), format='14A')\n",
    "    c3 = fits.Column(name='MEMPRSNT', format='L', array=included)\n",
    "    t = fits.BinTableHDU.from_columns([c1, c2, c3])\n",
    "    \n",
    "    # Writing the fits table\n",
    "    t.writeto(asn_root.lower()+'_asn.fits', overwrite=True)\n",
    "\n",
    "    print('Saved: {}'.format(asn_root.lower()+'_asn.fits' ), \"in the cwd directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read(\"lb3q01meq_asn.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = calcosF></a>\n",
    "## 1.3. Running the `calcos` pipeline\n",
    "\n",
    "Now we need to actually reduce the data using the `calcos` pipeline. If you have not already checked out our tutorial on [Running the `calcos` pipeline](), it contains vital information and is *highly recommended*.\n",
    "\n",
    "Unless we are connected to the STScI network, we will need to download the reference files and tell the pipeline where to look for the flat files, bad-pixel files, etc. \n",
    "\n",
    "In the next two cells, we setup an environment of reference files, download the files, and save the output of the crds download process in a log file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "%env lref ./data/reference/references/hst/cos/\n",
    "%env CRDS_SERVER_URL https://hst-crds.stsci.edu\n",
    "%env CRDS_PATH ./data/reference/\n",
    "\n",
    "# The next line depends on your context and pmap file \n",
    "!crds bestrefs --files data/*corrtag*.fits  --sync-references=2 --update-bestrefs --new-context 'hst_0836.pmap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir+'crds_output_1.txt', 'w') as f: # This file now contains the output of the last cell\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And now, for the pipeline itself:\n",
    "\n",
    "Again, *because we wish to compare against the unfiltered data*, we must run the pipeline twice:\n",
    "1. For these new **filtered** files\n",
    "2. For the original **unfiltered** files, so that we can compare and see the effect of filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# First, run with the \"FILTERED\" data with only time-tag datapoints allowed by the filter\n",
    "calcos.calcos('./filtered_asn.fits', outdir = output_dir + \"filtered_data_outs\", verbosity = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# Now, run calcos with the initial \"FULL\" data - with all time-tag datapoints\n",
    "calcos.calcos('./lb3q01meq_asn.fits', outdir = output_dir + \"full_data_outs\", verbosity = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = compF></a>\n",
    "# 2. Comparing the filtered and unfiltered data\n",
    "### Excellent! We're essentially done at this point. Let's just read in both of the processed spectra (the `x1dsum` files) and plot the spectra against one another.\n",
    "\n",
    "*You can ignore any `UnitsWarning`s that pop up about formatting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftab = Table.read(\"./output/full_data_outs/\"+filename_root+\"_x1dsum.fits\")['WAVELENGTH','FLUX', 'ERROR'] # read in the wvln, flux, flux error of the unfiltered spectrum file\n",
    "ttab = Table.read(\"./output/filtered_data_outs/filtered_x1dsum.fits\")['WAVELENGTH','FLUX', 'ERROR'] # read in the wvln, flux, flux error of the filtered spectrum file\n",
    "\n",
    "combo_dict_f = {'WAVELENGTH':[], 'FLUX':[], 'ERROR':[]}\n",
    "combo_dict_t = {'WAVELENGTH':[], 'FLUX':[], 'ERROR':[]}\n",
    "\n",
    "for row in ftab:\n",
    "    for key in row.colnames:\n",
    "        combo_dict_f[key]+=(list(row[key]))\n",
    "        \n",
    "for row in ttab:\n",
    "    for key in row.colnames:\n",
    "        combo_dict_t[key]+=(list(row[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15)) \n",
    "gs = gridspec.GridSpec(3,1, height_ratios=[3, 1,1]) \n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax2 = plt.subplot(gs[2])\n",
    "\n",
    "\n",
    "ax0.scatter(combo_dict_f[\"WAVELENGTH\"], combo_dict_f[\"FLUX\"], s = 2, alpha = 1, c = 'C1',label = \"Unfiltered\")\n",
    "ax0.scatter(combo_dict_t[\"WAVELENGTH\"], combo_dict_t[\"FLUX\"], s = 2, alpha = 1, c = 'C2', label = \"Filtered to SUN_ALT<0\")\n",
    "\n",
    "ax1.scatter(combo_dict_f[\"WAVELENGTH\"], combo_dict_f[\"FLUX\"], s = 2, alpha = 1, c = 'C1',label = \"Unfiltered\")\n",
    "ax0.set_xticks([])\n",
    "ax2.scatter(combo_dict_t[\"WAVELENGTH\"], combo_dict_t[\"FLUX\"], s = 2, alpha = 1, c = 'C2', label = \"Filtered to SUN_ALT<0\")\n",
    "ax1.set_xticks([])\n",
    "\n",
    "ax0.legend(fontsize = 20)\n",
    "ax1.legend(fontsize = 20)\n",
    "ax2.legend(fontsize = 20)\n",
    "\n",
    "ax2.set_xlabel(\"Wavelength [$\\AA$]\", fontsize = 30)\n",
    "ax0.set_title(\"Spectra of Filtered and Unfiltered by `SUN_ALT`\", size = 35)\n",
    "ax1.set_ylabel(\"Flux [$\\AA$]\", fontsize = 30, y=0.2, horizontalalignment='center')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(plots_dir + 'compare_spectra_sunalt.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the filtered spectrum largely follows the unfiltered spectrum; however, because in this case we filtered out a majority of the datapoints, we see a significant reduction in precision in flux space (visible as a **banding** in the y-axis). This comes about because with so few datapoints, the bands represent wavelengths which received (0,1,2...n) discrete photons. The banding is more pronounced at longer, redder wavelengths. \n",
    "\n",
    "Below, let's make one final plot: a chunk of the spectrum around \\~1550 Ã… showing the errors in flux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0,ax1) = plt.subplots(nrows=2, ncols=1, sharex='col',\n",
    "                               gridspec_kw={'height_ratios': [1, 1]},\n",
    "                               figsize=(15,8))\n",
    "\n",
    "ax0.errorbar(combo_dict_f[\"WAVELENGTH\"], combo_dict_f[\"FLUX\"], combo_dict_f[\"ERROR\"], \n",
    "             linestyle = '', markersize = 2, alpha = 1, c = 'C1',label = \"Unfiltered\")\n",
    "ax1.errorbar(combo_dict_t[\"WAVELENGTH\"], combo_dict_t[\"FLUX\"], combo_dict_t[\"ERROR\"], \n",
    "             linestyle = '', markersize = 2, alpha = 1, c = 'C2', label = \"Filtered to SUN_ALT<0\")\n",
    "\n",
    "ax0.legend(fontsize = 20)\n",
    "ax1.legend(fontsize = 20)\n",
    "\n",
    "\n",
    "ax1.set_xlabel(\"Wavelength [$\\AA$]\", fontsize = 30)\n",
    "ax1.set_ylabel(\"Flux [$\\AA$]\", fontsize = 30, y=1.05, horizontalalignment='center')\n",
    "ax0.set_xlim(1543,1563)\n",
    "ax1.set_xlim(ax0.get_xlim())\n",
    "ax1.set_ylim(ax0.get_ylim())\n",
    "ax0.set_title(\"Spectra with Errors of Filtered and Unfiltered by `SUN_ALT`\", size = 30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(plots_dir + 'ebar_compare_spectra_sunalt.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With substantially fewer datapoints, our filtered dataset has significantly larger errors. We can, however, understand why we might want to filter by this, or another `sun_alt` filter. For instance, if most of your exposure was taken at night, but the last 10% was taken after the sun had risen and induced an atmospheric line which interferes with your data, it would be necessary to filter out this last 10% of the exposure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You finished this notebook!\n",
    "### There are more COS data walkthrough notebooks on different topics. You can find them [here](https://github.com/spacetelescope/COS-Notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## About this Notebook\n",
    "**Author:** [Nat Kerman](nkerman@stsci.edu)\n",
    "\n",
    "**Contributors:** Elaine Mae Frazer\n",
    "\n",
    "**Updated On:** 2020-11-16\n",
    "\n",
    "> *This tutorial was generated to be in compliance with the [STScI style guides](https://github.com/spacetelescope/style-guides) and would like to cite the [Jupyter guide](https://github.com/spacetelescope/style-guides/blob/master/templates/example_notebook.ipynb) in particular.*\n",
    "\n",
    "## Citations\n",
    "\n",
    "If you use `astropy`, `matplotlib`, `astroquery`, or `numpy` for published research, please cite the\n",
    "authors. Follow these links for more information about citations:\n",
    "\n",
    "* [Citing `astropy`/`numpy`/`matplotlib`](https://www.scipy.org/citing.html)\n",
    "* [Citing `astroquery`](https://astroquery.readthedocs.io/en/latest/)\n",
    "\n",
    "---\n",
    "\n",
    "[Top of Page](#topD)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> \n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
